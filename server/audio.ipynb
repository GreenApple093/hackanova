{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ItqWDrZ9idc",
        "outputId": "b0b63dff-cd88-44d3-8340-d5a600e22859"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install required libraries (run this in your terminal or uncomment if running in a script)\n",
        "%pip install moviepy librosa transformers torch torchvision mediapipe opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj6cnoLO-ckF"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from moviepy import VideoFileClip\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import torch\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2H5X8If-e4V"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Define function to extract audio from video\n",
        "def extract_audio(video_path, output_audio_path=\"temp_audio.wav\"):\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(output_audio_path, codec='pcm_s16le')\n",
        "    return output_audio_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7dYiPwI-lnF"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Define function to process audio using Wav2Vec2\n",
        "def process_audio(audio_path):\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    waveform, sample_rate = librosa.load(audio_path, sr=16000)\n",
        "    inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    audio_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return audio_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOTQuejK-t6V"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Define function to extract visual features (lip movements)\n",
        "def extract_visual_features(video_path):\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
        "    video = VideoFileClip(video_path)\n",
        "    frames = [frame for frame in video.iter_frames()]\n",
        "    lip_landmarks = []\n",
        "    face_detection_success = 0\n",
        "    for frame in frames:\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_mesh.process(frame_rgb)\n",
        "        if results.multi_face_landmarks:\n",
        "            face_detection_success += 1\n",
        "            lip_points = [results.multi_face_landmarks[0].landmark[i] for i in range(0, 20)]\n",
        "            lip_coords = [(p.x, p.y, p.z) for p in lip_points]\n",
        "            lip_landmarks.append(np.array(lip_coords).flatten())\n",
        "    if lip_landmarks:\n",
        "        visual_embeddings = np.mean(lip_landmarks, axis=0)\n",
        "    else:\n",
        "        raise ValueError(\"No face detected in the video.\")\n",
        "    face_detection_rate = face_detection_success / len(frames)\n",
        "    return visual_embeddings, face_detection_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Define function to compute mismatch metrics\n",
        "def compute_mismatch_metrics(audio_embeddings, visual_embeddings):\n",
        "    # Normalize embeddings\n",
        "    audio_embeddings = audio_embeddings / np.linalg.norm(audio_embeddings)\n",
        "\n",
        "    # Pad visual embeddings to match audio embeddings size\n",
        "    padded_visual_embeddings = np.pad(\n",
        "        visual_embeddings,\n",
        "        (0, audio_embeddings.shape[0] - visual_embeddings.shape[0]),\n",
        "        mode='constant'\n",
        "    )\n",
        "    padded_visual_embeddings = padded_visual_embeddings / np.linalg.norm(padded_visual_embeddings)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cosine_similarity = np.dot(audio_embeddings, padded_visual_embeddings)\n",
        "    mismatch_score = 1 - cosine_similarity\n",
        "\n",
        "    # Compute Euclidean distance\n",
        "    euclidean_distance = np.linalg.norm(audio_embeddings - padded_visual_embeddings)\n",
        "\n",
        "    return {\n",
        "        \"cosine_similarity\": cosine_similarity,\n",
        "        \"mismatch_score\": mismatch_score,\n",
        "        \"euclidean_distance\": euclidean_distance\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm-i5LHW-z18"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Main function to analyze video\n",
        "def analyze_video(video_path):\n",
        "    # Step 1: Extract audio from video\n",
        "    audio_path = extract_audio(video_path)\n",
        "\n",
        "    # Step 2: Process audio to get embeddings\n",
        "    audio_embeddings = process_audio(audio_path)\n",
        "\n",
        "    # Step 3: Extract visual features (lip movements)\n",
        "    visual_embeddings, face_detection_rate = extract_visual_features(video_path)\n",
        "\n",
        "    # Step 4: Compute mismatch metrics\n",
        "    metrics = compute_mismatch_metrics(audio_embeddings, visual_embeddings)\n",
        "\n",
        "    # Clean up temporary files\n",
        "    if os.path.exists(audio_path):\n",
        "        os.remove(audio_path)\n",
        "\n",
        "    return metrics, face_detection_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzBNKSRs-_9t",
        "outputId": "4556e824-6969-4e3b-ccad-5ee2fdd0be7e"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Run analysis on a sample video and save results as JSON\n",
        "if __name__ == \"__main__\":\n",
        "    import json\n",
        "    \n",
        "    video_path = \"000471.mp4\"  # Replace with your video file path\n",
        "    metrics, face_detection_rate = analyze_video(video_path)\n",
        "    \n",
        "    # Add analysis result based on mismatch score\n",
        "    if metrics['mismatch_score'] < 0.5:\n",
        "        analysis_result = \"Audio and visual content are well-aligned.\"\n",
        "    elif 0.5 <= metrics['mismatch_score'] <= 1:\n",
        "        analysis_result = \"Moderate mismatch detected. Check for minor inconsistencies.\"\n",
        "    else:\n",
        "        analysis_result = \"High mismatch detected. Audio and visual content are inconsistent.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "# Define the path for the results.json file\n",
        "results_file_path = 'results.json'\n",
        "\n",
        "# Prepare the data to be saved\n",
        "results_data = {\n",
        "    \"face_detection_rate\": round(face_detection_rate * 100, 2),\n",
        "    \"cosine_similarity\": round(float(metrics['cosine_similarity']), 2),\n",
        "    \"mismatch_score\": round(float(metrics['mismatch_score']), 2),\n",
        "    \"euclidean_distance\": round(float(metrics['euclidean_distance']), 2),\n",
        "    \"analysis_result\": analysis_result\n",
        "}\n",
        "\n",
        "try:\n",
        "    # If the file exists and is not empty, read and update existing data\n",
        "    if os.path.exists(results_file_path) and os.path.getsize(results_file_path) > 0:\n",
        "        with open(results_file_path, 'r') as file:\n",
        "            existing_data = json.load(file)\n",
        "        # Update the existing data with new results\n",
        "        existing_data.update(results_data)\n",
        "        results_data = existing_data\n",
        "    \n",
        "    # Write the results to the file\n",
        "    with open(results_file_path, 'w') as file:\n",
        "        json.dump(results_data, file, indent=4)\n",
        "    \n",
        "    print(f\"Results saved to {results_file_path}: {results_data}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error handling results file: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
